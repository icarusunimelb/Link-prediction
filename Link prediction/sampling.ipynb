{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from math import log\n",
    "from networkx.algorithms import community\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23946602\n",
      "[[     18   17464]\n",
      " [     18   36718]\n",
      " [     18   38399]\n",
      " ...\n",
      " [4867036 4744223]\n",
      " [4867036 4762830]\n",
      " [4867036 4855480]]\n"
     ]
    }
   ],
   "source": [
    "# Read all the edges\n",
    "edge_list = []\n",
    "with open('./comp90051-2020-sem2-proj1/train.txt', 'r') as training_data_set:\n",
    "    for line in training_data_set:\n",
    "        line_array = line.split()\n",
    "        follower = int(line_array[0])\n",
    "        for i in range(1, len(line_array)):\n",
    "            following = int(line_array[i])\n",
    "            edge_list.append([follower, following])\n",
    "# Remove duplicate edge\n",
    "edge_list = np.array(edge_list)\n",
    "edge_list = np.unique(edge_list, axis=0)\n",
    "print(len(edge_list))\n",
    "print(edge_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[[3563811 3600160]\n",
      " [2052043 1401960]\n",
      " [4517994 1690636]\n",
      " ...\n",
      " [4242514 1413468]\n",
      " [ 555531 1290080]\n",
      " [1707829 2373045]]\n"
     ]
    }
   ],
   "source": [
    "# Generate the testing dataset\n",
    "# Read the test dataset\n",
    "test_list = []\n",
    "with open('./comp90051-2020-sem2-proj1/test-public.txt', 'r') as test_data_set:\n",
    "    for line in test_data_set:\n",
    "        line_array = line.split()\n",
    "        if line_array[0] == \"Id\":\n",
    "            continue\n",
    "        else:\n",
    "            test_list.append([int(line_array[1]), int(line_array[2])])\n",
    "test_list = np.array(test_list)\n",
    "print(len(test_list))\n",
    "print(test_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "source_list = []\n",
    "sink_list = []\n",
    "for edge in edge_list:\n",
    "    source_list.append(edge[0])\n",
    "    sink_list.append(edge[1])\n",
    "source_list = np.array(source_list)\n",
    "sink_list = np.array(sink_list)\n",
    "\n",
    "edge_df = pd.DataFrame({'Source': source_list, 'Sink': sink_list})\n",
    "edge_df.to_csv('./data_processing/edge_df.csv')\n",
    "\n",
    "test_source_list = []\n",
    "test_sink_list = []\n",
    "for edge in test_list:\n",
    "    test_source_list.append(edge[0])\n",
    "    test_sink_list.append(edge[1])\n",
    "test_source_list = np.array(test_source_list)\n",
    "test_sink_list = np.array(test_sink_list)\n",
    "\n",
    "test_df = pd.DataFrame({'Source': test_source_list, 'Sink': test_sink_list})\n",
    "test_df.to_csv('./data_processing/test_df.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This code is commented due to time consuming\n",
    "# Generate training data\n",
    "SAMPLING_SIZE = 5000\n",
    "edge_df_temp = edge_df.copy()\n",
    "total_nodes = len(set(edge_df_temp['Source']).union(set(edge_df_temp['Sink'])))\n",
    "# empty list to store removable links\n",
    "removable_links_index = []\n",
    "\n",
    "iterative_count = 0\n",
    "while len(removable_links_index) < SAMPLING_SIZE or iterative_count > SAMPLING_SIZE*100:\n",
    "    if iterative_count % 100 == 0:\n",
    "        print(len(removable_links_index))\n",
    "        print(len(set(edge_df_temp['Source']).union(set(edge_df_temp['Sink']))))\n",
    "    i = random.choice(edge_df_temp.index)\n",
    "    if i not in removable_links_index:\n",
    "        edge_df_temp_temp = edge_df_temp.drop(index = i)\n",
    "        if len(set(edge_df_temp_temp['Source']).union(set(edge_df_temp_temp['Sink']))) == total_nodes:\n",
    "            removable_links_index.append(i)\n",
    "            edge_df_temp = edge_df_temp_temp\n",
    "    iterative_count += 1\n",
    "\n",
    "removable_links_index = np.array(removable_links_index)\n",
    "print(\"removable_links_index: \"+str(len(removable_links_index)))\n",
    "positive_index = removable_links_index[0:SAMPLING_SIZE]\n",
    "positive_data = edge_df[edge_df.index.isin(positive_index)]\n",
    "positive_data = positive_data.iloc[0:SAMPLING_SIZE,:]\n",
    "graph_df = edge_df.iloc[~edge_df.index.isin(positive_index)]\n",
    "graph_df.to_csv('./data_processing/graph_df.csv')\n",
    "\n",
    "print(len(positive_data))\n",
    "print(positive_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4867136\n"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(edge_df, \"Source\", \"Sink\", create_using=nx.Graph())\n",
    "# Expected 4867136\n",
    "print(len(G.nodes()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "          Source     Sink\n",
      "365          135  3648164\n",
      "470          177  4651182\n",
      "893          873  2180497\n",
      "1676         877  1505989\n",
      "1900         877  1916061\n",
      "...          ...      ...\n",
      "5308100  1250021  3348219\n",
      "5308386  1250021  3365154\n",
      "5308404  1250021  3365832\n",
      "5308805  1250021  3387138\n",
      "5309349  1250021  3419523\n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "10000\n",
      "        Source     Sink\n",
      "2        69534   910199\n",
      "3      2197375   861609\n",
      "5      2150261    84224\n",
      "6       441734  3668648\n",
      "13     1192480  4726096\n",
      "...        ...      ...\n",
      "25973  1868397  3611021\n",
      "25974  1736217  3445604\n",
      "25976   435064  1235816\n",
      "25978  4159765    14428\n",
      "25981   182728  3013905\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate training data\n",
    "SAMPLING_SIZE = 10000\n",
    "positive_index = random.sample(range(0, edge_df.shape[0]), SAMPLING_SIZE*5)\n",
    "x=edge_df.Sink.value_counts()==1\n",
    "single_appearance= set(x[x==True].index)\n",
    "\n",
    "# Exclude the edge, the removal of which may lead to isolated node\n",
    "# Then in the feature extraction, remove the edge before processing this edge\n",
    "# and add back after processing this edge, which can reduce the information leak\n",
    "positive_data = edge_df[edge_df.index.isin(positive_index) & ~edge_df.Sink.isin(single_appearance)]\n",
    "positive_data = positive_data.iloc[0:SAMPLING_SIZE,:]\n",
    "print(len(positive_data))\n",
    "print(positive_data)\n",
    "\n",
    "# get negative data\n",
    "neg_sample_source = random.sample(list(edge_df.Source), SAMPLING_SIZE*5)\n",
    "neg_sample_sink = random.sample(list(edge_df.Sink), SAMPLING_SIZE*5)\n",
    "\n",
    "neg_sample_candidate1 = pd.DataFrame({'Source':neg_sample_source, 'Sink':neg_sample_sink})\n",
    "\n",
    "common = edge_df.merge(neg_sample_candidate1,on=['Source','Sink'])\n",
    "neg_sample_candidate2 = neg_sample_candidate1[(~neg_sample_candidate1.Source.isin(common.Source))&(~neg_sample_candidate1.Sink.isin(common.Sink))]\n",
    "\n",
    "common2 = test_df.merge(neg_sample_candidate2,on=['Source','Sink'])\n",
    "neg_sample_candidate3 = neg_sample_candidate2[(~neg_sample_candidate2.Source.isin(common2.Source))&(~neg_sample_candidate2.Sink.isin(common2.Sink))]\n",
    "\n",
    "coincide_df = pd.DataFrame({'Source': G.nodes(), 'Sink': G.nodes()})\n",
    "common3 = coincide_df.merge(neg_sample_candidate3,on=['Source','Sink'])\n",
    "negative_data = neg_sample_candidate3[(~neg_sample_candidate3.Source.isin(common3.Source))&(~neg_sample_candidate3.Sink.isin(common3.Sink))]\n",
    "\n",
    "negative_data = negative_data.iloc[0:SAMPLING_SIZE,:]\n",
    "print(len(negative_data))\n",
    "print(negative_data)\n",
    "\n",
    "negative_data['Label']=0\n",
    "positive_data['Label']=1\n",
    "positive_data.to_csv('./data_processing/positive_data.csv')\n",
    "negative_data.to_csv('./data_processing/negative_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# generate community\n",
    "communities = list(community.asyn_fluidc(G,100))\n",
    "print(len(communities))\n",
    "np.save('./data_processing/communities.npy', communities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}